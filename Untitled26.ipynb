import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback
from collections import deque
import random
import os

# --- 1. Data Loading (ADAPTED FOR CDS FILE STRUCTURE) ---
def load_cds_data(file_path, target_index_name=None):
    """
    Loads historical CDS price (spread) data from a CSV file.
    Assumes the CSV has 'Date', 'IndexName', and 'Spread' columns.
    Filters by target_index_name if provided.
    """
    try:
        df = pd.read_csv(file_path, parse_dates=['Date'])

        # Filter by IndexName if a specific one is provided
        if target_index_name:
            if 'IndexName' not in df.columns:
                raise ValueError("CSV must contain an 'IndexName' column for filtering.")
            df_filtered = df[df['IndexName'] == target_index_name].copy()
            if df_filtered.empty:
                raise ValueError(f"No data found for IndexName: '{target_index_name}'. "
                                 f"Available indices: {df['IndexName'].unique().tolist()}")
            df = df_filtered

        # Ensure 'Date' is set as index and sort
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True) # Ensure chronological order

        # Rename your CDS spread column to 'price' for consistency with the RSI calculation
        if 'Spread' in df.columns:
            df = df[['Spread']].copy()
            df.rename(columns={'Spread': 'price'}, inplace=True)
        elif 'Price' in df.columns: # Fallback if your column is named 'Price'
            df = df[['Price']].copy()
            df.rename(columns={'Price': 'price'}, inplace=True)
        else:
            raise ValueError("Expected 'Spread' or 'Price' column in your CDS data after filtering.")

        # Handle missing data robustly
        initial_na_count = df.isna().sum().sum()
        if initial_na_count > 0:
            print(f"Warning: Found {initial_na_count} NaN values. Attempting to fill.")
            df.ffill(inplace=True) # Forward fill any missing data
            df.bfill(inplace=True) # Backward fill remaining missing data
            df.dropna(inplace=True) # Drop any rows that still have NaNs after ffill/bfill
            if df.isna().sum().sum() > 0:
                print("Warning: NaNs still present after ffill/bfill/dropna. This might indicate issues.")

        if df.empty:
            raise ValueError("DataFrame is empty after loading and preprocessing. Check your data file.")

        print(f"Loaded {len(df)} data points from {file_path}")
        if target_index_name:
            print(f"Filtered for IndexName: '{target_index_name}'")
        print("CDS Data Head:\n", df.head())
        print("CDS Data Tail:\n", df.tail())
        return df

    except FileNotFoundError:
        print(f"Error: Data file not found at {file_path}. Please provide a valid path.")
        exit() # Exit if data not found, as the environment won't work
    except Exception as e:
        print(f"Error loading or processing data: {e}")
        exit()

# --- 2. Technical Indicator Calculation ---
def calculate_rsi(data, window=14):
    """
    Calculates the Relative Strength Index (RSI).
    'data' should be a DataFrame with a 'price' column (which is your CDS spread).
    """
    diff = data['price'].diff()
    gain = diff.mask(diff < 0, 0)
    loss = -diff.mask(diff > 0, 0)

    avg_gain = gain.ewm(com=window - 1, min_periods=window).mean()
    avg_loss = loss.ewm(com=window - 1, min_periods=window).mean()

    # Handle division by zero for rs, if avg_loss is 0
    with np.errstate(divide='ignore', invalid='ignore'):
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        # Fill NaNs with a neutral value (e.g., 50) and handle infinities gracefully
        rsi = rsi.replace([np.inf, -np.inf], np.nan).fillna(50)
    return rsi


# --- 3. Custom Gym Environment for Trading ---
class CreditRSIEnv(gym.Env):
    """
    A custom Gymnasium environment for trading credit indexes (CDS spreads) using RSI.
    The agent learns when to buy/sell protection or hold based on CDS spread and RSI.
    """
    metadata = {'render_modes': ['human'], 'render_fps': 30}

    def __init__(self, df, window=14, initial_balance=10000, transaction_cost_rate=0.0005, render_mode=None,
                 fixed_cs01_per_unit=100000): # USD change per 1bp spread move per unit
        super().__init__()
        self.df = df.copy() # Operate on a copy to avoid modifying original dataframe
        self.window = window
        self.initial_balance = initial_balance
        self.transaction_cost_rate = transaction_cost_rate

        # DV01 (or CS01 for Credit) per unit of exposure.
        # This means if shares_held = 1, and spread moves 1bp, P&L is fixed_cs01_per_unit.
        # If spread is in basis points already, this works directly.
        self.dv01_per_unit = fixed_cs01_per_unit

        # Calculate RSI and other features
        self.df['rsi'] = calculate_rsi(self.df, self.window)
        # Daily spread change in basis points
        self.df['spread_change'] = self.df['price'].diff().fillna(0)

        # Drop NaN values introduced by RSI calculation and any initial NaNs
        original_len = len(self.df)
        self.df.dropna(inplace=True)
        self.df.reset_index(drop=True, inplace=True) # Reset index after dropping rows

        if self.df.empty:
            raise ValueError("DataFrame is empty after preprocessing (RSI/spread_change). Check your data and window size.")
        if len(self.df) < self.window + 2:
             raise ValueError(f"Not enough data points ({len(self.df)}) after preprocessing for window size {self.window}.")


        self.current_step = 0
        self.max_steps = len(self.df) - 1 # Max index to access data

        # Action Space: 0 = Hold, 1 = Buy Protection, 2 = Sell Protection
        # Buy Protection: Bet on spreads widening (credit worsening)
        # Sell Protection: Bet on spreads tightening (credit improving)
        self.action_space = spaces.Discrete(3)

        # Define Observation Space dynamically based on actual data ranges
        # This makes the environment more robust to different datasets
        _max_shares = 10 # Example max shares: agent can hold up to 10 units long or short
        _min_balance = -self.initial_balance * 0.5 # Allow some drawdown before termination
        _max_balance = self.initial_balance * 5 # Allow significant growth

        low_bounds = np.array([
            self.df['price'].min() * 0.9,      # Current Spread (allow some buffer)
            0.0,                               # RSI (min 0)
            -_max_shares,                      # Shares Held (negative for short protection)
            _min_balance,                      # Balance (can dip below 0 if not terminated)
            self.df['spread_change'].min()     # Spread Change
        ])
        high_bounds = np.array([
            self.df['price'].max() * 1.1,      # Current Spread (allow some buffer)
            100.0,                             # RSI (max 100)
            _max_shares,                       # Shares Held (positive for long protection)
            _max_balance,                      # Balance (can grow)
            self.df['spread_change'].max()     # Spread Change
        ])

        # It's crucial that observation space bounds encapsulate *all possible* values.
        # Normalization handled internally by StableBaselines3's VecNormalize or custom logic.
        self.observation_space = spaces.Box(low=low_bounds, high=high_bounds, dtype=np.float32)

        # Environment State Variables
        self.balance = self.initial_balance
        self.shares_held = 0 # Positive: Long Protection, Negative: Short Protection
        self.episode_history = [] # To store details for rendering/analysis

        self.render_mode = render_mode
        self.fig = None
        self.ax1 = None # Using ax1 and ax2 for multiple plots
        self.ax2 = None


    def _get_obs(self):
        # Current data point for observation
        current_data = self.df.iloc[self.current_step]
        current_spread = current_data['price']
        rsi = current_data['rsi']
        spread_change = current_data['spread_change']

        obs = np.array([
            current_spread,
            rsi,
            self.shares_held,
            self.balance,
            spread_change
        ], dtype=np.float32)

        # No manual normalization here if VecNormalize wrapper is used with make_vec_env.
        # If not using VecNormalize, and relying on `spaces.Box` ranges,
        # ensure your NN can handle unnormalized inputs or normalize before passing to model.predict().
        # For simplicity with SB3, it's often better to let VecNormalize handle it.
        # If you *must* normalize here:
        # normalized_obs = (obs - self.observation_space.low) / (self.observation_space.high - self.observation_space.low)
        # return normalized_obs
        return obs # Return unnormalized observation, let SB3 or internal policies handle it.


    def step(self, action):
        done = False
        reward = 0
        truncated = False # Gymnasium added this for episodes ending due to time limits, etc.

        # Store spread at the beginning of the step to calculate P&L for the day
        previous_spread = self.df.iloc[self.current_step]['price']
        
        # Advance the step counter
        self.current_step += 1

        # Check if episode ends due to reaching max_steps
        if self.current_step > self.max_steps: # Use > to catch the last step correctly
            done = True
            # No further P&L or transactions, just wrap up
            observation = self._get_obs() # Get last observation
            info = {"final_balance": self.balance}
            return observation, reward, done, truncated, info

        # Get current spread after advancing step
        current_spread = self.df.iloc[self.current_step]['price']

        # Calculate spread change in basis points
        spread_change_bps = current_spread - previous_spread
        
        # Calculate P&L from existing open positions based on fixed CS01 per unit
        # P&L = shares_held * spread_change_bps * fixed_cs01_per_unit
        pnl_from_open_positions = self.shares_held * spread_change_bps * self.dv01_per_unit

        # Apply daily P&L to balance
        self.balance += pnl_from_open_positions
        
        # --- Transaction Logic ---
        # Transaction cost is a fixed percentage of the *CS01* per unit traded.
        transaction_cost_per_unit = self.dv01_per_unit * self.transaction_cost_rate

        # Flags to track if a transaction occurred for reward shaping
        transaction_made = False

        if action == 1:  # Buy Protection (Go Long, or increase Long position)
            # Check if balance can cover the transaction cost
            if self.balance >= transaction_cost_per_unit:
                self.balance -= transaction_cost_per_unit
                self.shares_held += 1
                transaction_made = True
            # else: Cannot afford, implicitly acts as hold for this specific transaction, no balance change.
            # No penalty for inability to transact, just no change in position.

        elif action == 2:  # Sell Protection (Go Short, or decrease Long position)
            # If current shares held are positive (long position), first try to close it.
            # If shares held are zero or negative, then allow going further short,
            # but only if balance allows for transaction cost.
            if self.shares_held > 0: # Closing a long position
                self.balance -= transaction_cost_per_unit
                self.shares_held -= 1
                transaction_made = True
            elif self.shares_held <= 0 and self.balance >= transaction_cost_per_unit: # Allowing new short or increase short
                self.balance -= transaction_cost_per_unit
                self.shares_held -= 1
                transaction_made = True
            # else: Cannot afford to go further short/no long position to close, implicitly acts as hold.


        # Reward is primarily based on P&L from positions
        reward = pnl_from_open_positions
        
        # --- Reward Shaping / Penalties ---
        # Penalize large drawdowns or negative balance
        if self.balance < 0:
            done = True
            reward -= self.initial_balance * 0.5 # Significant penalty for ruin

        # End episode if balance drops below a threshold (e.g., 50% of initial)
        elif self.balance < self.initial_balance * 0.5: # Use elif to avoid double penalizing if balance is < 0
            done = True
            reward -= self.initial_balance * 0.1 # Penalty for significant drawdown

        # Optional: Small penalty for transaction costs to encourage fewer trades if not profitable
        # if transaction_made:
        #     reward -= transaction_cost_per_unit * 0.1 # A fraction of cost, or a fixed small penalty

        # Store episode history for rendering and analysis
        self.episode_history.append({
            'step': self.current_step,
            'price': current_spread,
            'rsi': self.df.iloc[self.current_step]['rsi'],
            'action': action,
            'balance': self.balance,
            'shares_held': self.shares_held,
            'reward': reward
        })

        observation = self._get_obs()
        info = {"balance": self.balance, "shares_held": self.shares_held, "current_spread": current_spread}

        return observation, reward, done, truncated, info

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.balance = self.initial_balance
        self.shares_held = 0
        self.episode_history = []

        # Determine a safe range for the start index
        # Ensure enough data points for RSI calculation (window) and for future steps
        min_start_index = self.window + 1 # Needs data for RSI + current day
        # To ensure the episode can run for a reasonable duration without ending immediately
        # E.g., at least 100 steps of data available after start
        max_start_index = self.max_steps - 100 
        
        if max_start_index < min_start_index:
            # Not enough data for a robust random start. Start at the earliest possible.
            self.current_step = min_start_index
            print(f"Warning: Data too short for full random start range. Starting at {self.current_step}.")
        else:
            self.current_step = random.randint(min_start_index, max_start_index)

        # Initialize previous_spread if needed, though P&L is calculated with current_step-1
        # For a clean reset, just ensure current_step is valid for _get_obs
        
        observation = self._get_obs()
        info = {"initial_balance": self.initial_balance}
        return observation, info

    def render(self):
        if self.render_mode == 'human':
            if self.fig is None:
                plt.ion() # Turn on interactive mode
                self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)
                self.fig.suptitle('CDS Spread Trading Agent Performance')

                self.ax1.set_ylabel('CDS Spread (bps)')
                self.ax1.grid(True, linestyle=':', alpha=0.7)

                self.ax2.set_xlabel('Time Step')
                self.ax2.set_ylabel('Value')
                self.ax2.grid(True, linestyle=':', alpha=0.7)

                self.prices_plot, = self.ax1.plot([], [], 'b-', label='CDS Spread')
                self.buy_scatter = self.ax1.scatter([], [], color='green', marker='^', s=100, label='Buy Protection (Long)')
                self.sell_scatter = self.ax1.scatter([], [], color='red', marker='v', s=100, label='Sell Protection (Short)')
                self.ax1.legend(loc='upper left')

                self.rsi_plot, = self.ax2.plot([], [], 'm--', label='RSI')
                self.balance_plot, = self.ax2.plot([], [], 'k-', label='Balance')
                self.ax2.legend(loc='upper left')

                # Add RSI overbought/oversold lines
                self.ax2.axhline(70, color='gray', linestyle='--', alpha=0.7, label='RSI Overbought')
                self.ax2.axhline(30, color='gray', linestyle='--', alpha=0.7, label='RSI Oversold')


            steps = [h['step'] for h in self.episode_history]
            prices = [h['price'] for h in self.episode_history]
            rsis = [h['rsi'] for h in self.episode_history]
            balances = [h['balance'] for h in self.episode_history]

            buy_steps = [h['step'] for h in self.episode_history if h['action'] == 1]
            buy_prices = [h['price'] for h in self.episode_history if h['action'] == 1]
            sell_steps = [h['step'] for h in self.episode_history if h['action'] == 2]
            sell_prices = [h['price'] for h in self.episode_history if h['action'] == 2]

            self.prices_plot.set_data(steps, prices)
            self.buy_scatter.set_offsets(np.c_[buy_steps, buy_prices])
            self.sell_scatter.set_offsets(np.c_[sell_steps, sell_prices])

            self.rsi_plot.set_data(steps, rsis)
            self.balance_plot.set_data(steps, balances)

            # Adjust plot limits dynamically
            if len(steps) > 0:
                self.ax1.set_xlim(min(steps) - 1, max(steps) + 1)
                self.ax2.set_xlim(min(steps) - 1, max(steps) + 1) # Keep x-axis aligned

                min_price = min(prices) if prices else 0
                max_price = max(prices) if prices else 100
                self.ax1.set_ylim(min_price * 0.95, max_price * 1.05)

                min_rsi = min(rsis) if rsis else 0
                max_rsi = max(rsis) if rsis else 100
                min_balance = min(balances + [self.initial_balance * 0.5]) # Ensure lower bound includes termination threshold
                max_balance = max(balances + [self.initial_balance * 1.5]) # Ensure upper bound accommodates some growth

                self.ax2.set_ylim(min(min_rsi * 0.95, min_balance * 0.95),
                                  max(max_rsi * 1.05, max_balance * 1.05))

            self.fig.canvas.draw()
            self.fig.canvas.flush_events()
            plt.pause(0.01) # Small pause for animation

    def close(self):
        if self.fig is not None:
            plt.close(self.fig)
            self.fig = None
            self.ax1 = None
            self.ax2 = None

# --- 4. Training Callback (Optional but useful) ---
class CustomCallback(BaseCallback):
    """
    A custom callback that prints rewards and balance during training and saves the model.
    """
    def __init__(self, verbose: int = 0):
        super().__init__(verbose)
        self.rewards = deque(maxlen=100) # Keep track of last 100 episode rewards
        self.final_balances = deque(maxlen=100) # Track final balance of episodes

    def _on_step(self) -> bool:
        # self.locals['dones'] is a list for VecEnv, check the first environment
        if self.locals['dones'][0]:
            episode_reward = self.locals['rewards'][0]
            self.rewards.append(episode_reward)
            # Retrieve the final balance from the environment that just finished an episode
            final_balance = self.training_env.get_attr('balance')[0]
            self.final_balances.append(final_balance)

            mean_reward = np.mean(self.rewards)
            mean_balance = np.mean(self.final_balances)

            # Log periodically or when a new episode finishes
            if self.n_calls % 1000 == 0 or len(self.rewards) == self.rewards.maxlen: # Also log when deque is full
                print(f"Step: {self.n_calls}, Episode Reward: {episode_reward:.2f}, Mean Reward (last {len(self.rewards)}): {mean_reward:.2f}, Final Balance (this episode): {final_balance:.2f}, Mean Balance (last {len(self.final_balances)} episodes): {mean_balance:.2f}")

        return True

# --- 5. Main Execution Block ---
if __name__ == "__main__":
    cds_data_file = 'cds_data_full.csv'
    target_cds_index = 'ITRAXX_EUROPE_S38' # CHANGE THIS to your desired index

    # Example: Create a dummy CSV for testing
    if not os.path.exists(cds_data_file):
        print(f"Creating dummy data for '{cds_data_file}' as it was not found.")
        dates = pd.to_datetime(pd.date_range(start='2010-01-01', periods=1500, freq='D'))
        
        # Simulate some spread movement
        spread1 = np.random.normal(70, 20, 1500).cumsum() * 0.05 + 80
        spread1 = np.maximum(10, spread1) # Spreads shouldn't go below 0, set a floor

        spread2 = np.random.normal(80, 10, 1500).cumsum() * 0.03 + 70
        spread2 = np.maximum(5, spread2)

        dummy_data1 = pd.DataFrame({'Date': dates, 'IndexName': 'ITRAXX_EUROPE_S38', 'Spread': spread1})
        dummy_data2 = pd.DataFrame({'Date': dates, 'IndexName': 'CDX_NA_IG_S39', 'Spread': spread2})

        dummy_data = pd.concat([dummy_data1, dummy_data2]).sort_values(by='Date')
        dummy_data.to_csv(cds_data_file, index=False)
        print("Dummy data created.")

    # Load and preprocess data
    historical_cds_df = load_cds_data(cds_data_file, target_index_name=target_cds_index)

    # Initialize environment for training
    # fixed_cs01_per_unit: If 1 unit of 'shares_held' represents $100,000 notional, and 1bp move
    # affects this by 1/10000 of notional, then CS01 is $10. For a $100,000 notional, 1bp is $10.
    # If your spread is already in basis points, and 'dv01_per_unit' is the dollar value per share per 1bp,
    # then this setup is correct. For a standard CDS, $100k notional, 5yr maturity, a 1bp spread move
    # changes value by roughly $50-$70. Let's use 60 for example.
    # So if 1 unit is $100k notional: fixed_cs01_per_unit = 60
    # If you intend `dv01_per_unit` to be a much larger number like 100,000, it implies a
    # very large sensitivity, meaning a 1bp move results in $100,000 P&L per share.
    # This might be too volatile for training. Recheck your intended CS01 value.
    # For now, I'll stick to your 100,000, assuming it's the intended sensitivity.
    cs01_per_unit_value = 100000 # Your chosen value (e.g., if 1 unit represents a very large notional)

    # Consider using VecNormalize for better training stability with PPO if observations are not strictly bounded.
    # from stable_baselines3.common.vec_env import VecNormalize
    # env = CreditRSIEnv(df=historical_cds_df, window=14, initial_balance=10000,
    #                    transaction_cost_rate=0.0005, render_mode=None,
    #                    fixed_cs01_per_unit=cs01_per_unit_value)
    # vec_env = make_vec_env(lambda: env, n_envs=4)
    # vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.) # Normalize observations and rewards

    # Using your current setup without VecNormalize for consistency with provided code structure
    env = CreditRSIEnv(df=historical_cds_df, window=14, initial_balance=10000,
                       transaction_cost_rate=0.0005, render_mode=None,
                       fixed_cs01_per_unit=cs01_per_unit_value)
    vec_env = make_vec_env(lambda: env, n_envs=4) # Still beneficial for PPO training speed

    model = PPO("MlpPolicy", vec_env, verbose=1, learning_rate=0.0003, n_steps=2048,
                batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95,
                ent_coef=0.01, # Encourage exploration
                policy_kwargs=dict(net_arch=dict(pi=[64, 64], vf=[64, 64])),
                tensorboard_log="./ppo_credit_cds_tensorboard/")

    callback = CustomCallback(verbose=1)

    print(f"\n--- Starting training for {target_cds_index} ---")
    total_timesteps = 500000 # Increased for potentially better convergence
    model.learn(total_timesteps=total_timesteps, callback=callback)
    print("--- Training complete ---")

    model_save_path = f"ppo_credit_cds_agent_{target_cds_index.replace(' ', '_')}.zip"
    model.save(model_save_path)
    print(f"Model saved as {model_save_path}")

    # --- 6. Evaluate the trained agent ---
    print(f"\n--- Evaluating the trained agent for {target_cds_index} ---")
    
    # Use a separate evaluation set (e.g., last 20% of data)
    train_split_idx = int(len(historical_cds_df) * 0.8)
    eval_df = historical_cds_df.iloc[train_split_idx:].copy().reset_index(drop=True)

    if eval_df.empty or len(eval_df) < env.window + 2:
        # Fallback for very short datasets
        print("Warning: Not enough data for a proper evaluation split. Using last 200 data points or available if less.")
        eval_df = historical_cds_df.iloc[-min(200, len(historical_cds_df) - env.window - 1):].copy().reset_index(drop=True)
        if eval_df.empty or len(eval_df) < env.window + 2:
            raise ValueError("Evaluation DataFrame is still too small after slicing. Please provide more data or reduce window size.")

    # Initialize evaluation environment with render_mode='human'
    eval_env = CreditRSIEnv(df=eval_df, window=14, initial_balance=10000,
                            transaction_cost_rate=0.0005, render_mode='human',
                            fixed_cs01_per_unit=cs01_per_unit_value)

    obs, info = eval_env.reset()
    done = False
    total_reward = 0
    num_steps = 0

    while not done:
        action, _states = model.predict(obs, deterministic=True) # Use deterministic=True for evaluation
        obs, reward, done, truncated, info = eval_env.step(action)
        total_reward += reward
        num_steps += 1
        eval_env.render() # Render each step

        # Stop condition for evaluation: Either env reports done or reached end of eval data
        # Ensure it doesn't run past the available data in eval_df
        if num_steps >= len(eval_env.df) - eval_env.window - 1: # -1 for 0-indexing
            done = True

    eval_env.close() # Close the rendering window
    print(f"\nEvaluation Results for {target_cds_index}:")
    print(f"Total Steps: {num_steps}")
    print(f"Final Balance: ${eval_env.balance:.2f}")
    print(f"Total Reward: {total_reward:.2f}")
    profit_loss = eval_env.balance - eval_env.initial_balance
    print(f"Profit/Loss: ${profit_loss:.2f}")

    if eval_env.initial_balance > 0:
        percentage_gain_loss = (profit_loss / eval_env.initial_balance) * 100
        print(f"Percentage Gain/Loss: {percentage_gain_loss:.2f}%")
    else:
        print("Initial balance is zero, cannot calculate percentage gain/loss.")

    # Plot final balance history from evaluation
    eval_history_df = pd.DataFrame(eval_env.episode_history)
    if not eval_history_df.empty:
        plt.figure(figsize=(12, 6))
        plt.plot(eval_history_df['step'], eval_history_df['balance'], label='Balance')
        plt.axhline(y=eval_env.initial_balance, color='r', linestyle='--', label='Initial Balance')
        plt.title(f'Balance Evolution during Evaluation for {target_cds_index}')
        plt.xlabel('Time Step')
        plt.ylabel('Balance ($)')
        plt.legend()
        plt.grid(True)
        plt.show()

