import numpy as np
import pandas as pd
import random
from collections import deque
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import talib # For technical indicators

# Ensure reproducibility for consistent results
np.random.seed(42)
tf.random.set_seed(42)

# --- 1. Define the Trading Environment ---
class CDSTradingEnv:
    def __init__(self, data, target_index_col, initial_capital=100000, commission_rate=0.0001, spread_cost=0.0005):
        """
        Initializes the CDS Trading Environment, treating spreads as "prices" inversely.

        Args:
            data (pd.DataFrame): Historical data for CDS indexes with 'Date' as index
                                 and columns like 'CDX_IG', 'CDX_EM', etc. (spreads in bps).
            target_index_col (str): The name of the column in 'data' that represents the
                                    CDS index to be traded (e.g., 'CDX_IG').
            initial_capital (float): Starting capital for the agent.
            commission_rate (float): Commission per trade as a percentage of trade value.
            spread_cost (float): Represents bid-ask spread or slippage as a percentage.
        """
        if target_index_col not in data.columns:
            raise ValueError(f"'{target_index_col}' not found in data columns. Available: {data.columns.tolist()}")

        self.data = data.copy() # Work on a copy to avoid modifying original df
        self.target_index_col = target_index_col
        self.initial_capital = initial_capital
        self.capital = initial_capital
        self.commission_rate = commission_rate
        self.spread_cost = spread_cost
        self.shares = 0  # Number of CDS index units held (conceptual, represents exposure)
        
        # We trade based on the inverse of the spread:
        # Lower spread (tightening) = Higher "effective price" for a long position.
        # Higher spread (widening) = Lower "effective price" for a long position.
        # We use a simple inverse: 1 / spread, scaled to a reasonable number.
        # Or you can think of it as a value that increases when spreads tighten.
        # Let's use a normalized inverse spread for simplicity in price tracking.
        # You might also consider max_spread - current_spread.
        
        # Max spread can be a large number if spreads are very low, causing issues.
        # A simple linear transformation based on a min/max spread range might be better.
        # For now, let's treat the spread value itself directly.
        # Buying when spread is low and selling when spread is high means you are SHORTING the spread,
        # or buying PROTECTION when spread is low and selling PROTECTION when spread is high.
        # If we "buy low, sell high" on the spread value itself, it means we are profiting from spreads WIDENING.
        # If we want to profit from spreads TIGHTENING (typical long CDS position), then "buy low, sell high"
        # refers to the *implied price* of the CDS index.
        
        # Let's define the "price" that we are trading.
        # When spreads go down (tighten), a long CDS position gains value.
        # So, our "price" should increase when the spread decreases.
        # A simple transformation: MAX_SPREAD - CURRENT_SPREAD (scaled)
        
        # Calculate MAX_SPREAD for the whole dataset for normalization
        self.max_spread = self.data[self.target_index_col].max()
        # Define the "tradable price" for the agent: higher when spreads are tighter.
        # A small constant added to avoid division by zero or very large numbers if spread is near zero
        self.data['Tradable_Price'] = self.max_spread - self.data[self.target_index_col] + 1.0 # +1 to avoid zeros
        
        self.price_history = self.data['Tradable_Price'].values
        self.spread_history = self.data[self.target_index_col].values # Keep original spreads for state features

        self.current_step = 0
        self.history = [] # To store actions, capital, and position
        self.position = 0 # 0: Hold, 1: Long, -1: Short (simplified for now, refers to "Tradable_Price")

        self.window_size = 20 # Increased window for indicators
        self.action_space = [0, 1, 2] # 0: Hold, 1: Buy (long "Tradable_Price"), 2: Sell (flatten long position)

        # Pre-calculate technical indicators and return horizons
        self._prepare_features()

        self.reset()

    def _prepare_features(self):
        close_prices = self.data[self.target_index_col].values
        
        # Return Horizons (on the raw spread data)
        self.data['Return_1D'] = self.data[self.target_index_col].pct_change(1)
        self.data['Return_1W'] = self.data[self.target_index_col].pct_change(5) # 5 trading days in a week
        self.data['Return_2W'] = self.data[self.target_index_col].pct_change(10) # 10 trading days
        self.data['Return_1M'] = self.data[self.target_index_col].pct_change(20) # 20 trading days

        # Technical Indicators (on the raw spread data)
        # RSI: Use close_prices (spreads) directly
        # A low RSI on spreads means spreads are oversold (have fallen significantly) -> potential tightening
        # A high RSI on spreads means spreads are overbought (have risen significantly) -> potential widening
        self.data['RSI'] = talib.RSI(close_prices, timeperiod=14) # Default 14 periods

        # MACD: Use close_prices (spreads) directly
        # Positive MACD suggests upward momentum (spreads widening)
        # Negative MACD suggests downward momentum (spreads tightening)
        macd, macdsignal, macdhist = talib.MACD(close_prices, 
                                                 fastperiod=12, slowperiod=26, signalperiod=9)
        self.data['MACD'] = macd
        self.data['MACD_Signal'] = macdsignal
        self.data['MACD_Hist'] = macdhist

        # Fill NaNs created by technical indicators and returns (at the start of series)
        self.data.fillna(method='bfill', inplace=True) # Backfill NaNs to avoid losing early data points
        self.data.fillna(0, inplace=True) # Fill any remaining NaNs with 0

        # Features to be included in the state vector
        self.feature_cols = [
            'Tradable_Price', # Current "price"
            'Return_1D', 'Return_1W', 'Return_2W', 'Return_1M',
            'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist'
        ]
        
        # Scale features, this is important for neural networks
        # We'll use a min-max scaler based on the entire dataset
        self.scalers = {}
        for col in self.feature_cols:
            min_val = self.data[col].min()
            max_val = self.data[col].max()
            if max_val - min_val != 0:
                self.data[col + '_Scaled'] = (self.data[col] - min_val) / (max_val - min_val)
                self.scalers[col] = {'min': min_val, 'max': max_val}
            else:
                self.data[col + '_Scaled'] = 0.0 # If all values are the same
                self.scalers[col] = {'min': min_val, 'max': max_val} # Keep record
        
        self.scaled_feature_cols = [col + '_Scaled' for col in self.feature_cols]
        
        # Adjust starting step for window_size and largest indicator period (MACD slowperiod=26)
        # Max lookback needed: max(window_size, MACD_slowperiod, etc.)
        self.start_step_offset = max(self.window_size, 26) # Or 20 for 1-month return

        self.price_history = self.data['Tradable_Price'].values # Update with the new tradable price
        
        print(f"Features prepared. State size will be based on {len(self.scaled_feature_cols) + 1} features * {self.window_size} + 1 (position).")

    def reset(self):
        self.capital = self.initial_capital
        self.shares = 0
        self.current_step = self.start_step_offset # Start after enough data for all features and window
        self.history = []
        self.position = 0
        return self._get_state()

    def _get_state(self):
        if self.current_step < self.start_step_offset:
            # Not enough data for the initial window and indicators
            return np.zeros(len(self.scaled_feature_cols) * self.window_size + 1)
        
        # Get historical data for the window_size for all features
        state_window_data = self.data.loc[self.data.index[self.current_step - self.window_size : self.current_step], self.scaled_feature_cols].values
        
        # Flatten the window data and append current position
        state = state_window_data.flatten()
        state = np.append(state, self.position)
        return state

    def step(self, action):
        current_tradable_price = self.price_history[self.current_step]
        current_spread = self.spread_history[self.current_step] # For reference
        reward = 0
        done = False

        self.current_step += 1
        if self.current_step >= len(self.price_history):
            done = True

        # --- Execute Action ---
        if action == 1:  # Buy (go long on "Tradable_Price", profit if spreads tighten)
            if self.position == 0: # Only buy if currently flat
                buy_amount = self.capital / (current_tradable_price * (1 + self.commission_rate + self.spread_cost))
                if buy_amount * current_tradable_price > 0: # Ensure positive purchase
                    self.shares += buy_amount
                    self.capital -= buy_amount * current_tradable_price * (1 + self.commission_rate + self.spread_cost)
                    self.position = 1
        
        elif action == 2:  # Sell (liquidate long position on "Tradable_Price")
            if self.position == 1 and self.shares > 0: # Only sell if currently long
                sale_value = self.shares * current_tradable_price * (1 - self.commission_rate - self.spread_cost)
                
                # Reward based on percentage profit/loss on the closed trade
                # Need to track the entry price for more accurate PnL
                entry_info = next((item for item in reversed(self.history) if item['action'] == 1 and item['position'] == 1), None)
                if entry_info:
                    entry_tradable_price = entry_info['price']
                    trade_pnl = (current_tradable_price - entry_tradable_price) * self.shares
                    # Scale reward by initial capital to normalize
                    reward = trade_pnl / self.initial_capital * 10.0 # Scale for better signal
                    
                    # Add small penalty for losing trades to encourage better exits
                    if trade_pnl < 0:
                        reward -= 0.5 
                else:
                    reward = 0 # No clear entry found

                self.capital += sale_value
                self.shares = 0
                self.position = 0
        
        # --- Calculate Next State & End of Episode Reward ---
        next_state = self._get_state()

        if done:
            final_value = self.capital + self.shares * current_tradable_price * (1 - self.commission_rate)
            reward += (final_value - self.initial_capital) / self.initial_capital * 100.0 # Large reward for episode end performance
            
            # Penalize remaining open positions at the end if they are losing
            if self.shares != 0:
                # Approximate value of current holding
                current_hold_value = self.shares * current_tradable_price 
                # If still holding shares, calculate potential PnL relative to initial capital allocated
                if self.position == 1: # If long, penalize if tradable price fell
                    # Very rough estimate of holding cost
                    initial_allocation = self.initial_capital - self.capital # How much was invested
                    if initial_allocation > 0:
                         pnl_on_hold = (current_hold_value - initial_allocation) / initial_allocation
                         if pnl_on_hold < 0:
                            reward += pnl_on_hold * 5.0 # Penalize negative holding PnL
            
            # Significant penalty for overall negative performance
            if final_value < self.initial_capital:
                reward -= (self.initial_capital - final_value) / self.initial_capital * 20.0 
            
        # Record history for backtesting visualization
        self.history.append({
            'capital': self.capital,
            'shares': self.shares,
            'position': self.position,
            'spread': current_spread, # Store original spread
            'tradable_price': current_tradable_price, # Store the derived tradable price
            'action': action,
            'reward': reward,
            'portfolio_value': self.get_portfolio_value()
        })

        return next_state, reward, done, {}

    def get_portfolio_value(self):
        current_tradable_price = self.price_history[self.current_step-1] if self.current_step > 0 else self.price_history[0]
        return self.capital + self.shares * current_tradable_price


# --- 2. Implement the Q-Learning Agent (No changes needed here) ---
class DQNAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.95,
                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,
                 batch_size=64, memory_size=2000):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=memory_size)
        self.learning_rate = learning_rate
        self.gamma = gamma    # Discount factor for future rewards
        self.epsilon = epsilon  # Exploration-exploitation trade-off
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential()
        model.add(Dense(128, input_dim=self.state_size, activation='relu')) # Increased neurons
        model.add(Dense(64, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size) # Explore
        
        state_reshaped = state.reshape(1, -1) 
        act_values = self.model.predict(state_reshaped, verbose=0)
        return np.argmax(act_values[0]) # Exploit

    def replay(self):
        if len(self.memory) < self.batch_size:
            return

        minibatch = random.sample(self.memory, self.batch_size)
        states, targets = [], []
        
        for state, action, reward, next_state, done in minibatch:
            state_reshaped = state.reshape(1, -1)
            next_state_reshaped = next_state.reshape(1, -1)
            
            target = reward
            if not done:
                target = reward + self.gamma * np.amax(self.model.predict(next_state_reshaped, verbose=0)[0])
            
            current_q_values = self.model.predict(state_reshaped, verbose=0)[0]
            current_q_values[action] = target
            
            states.append(state)
            targets.append(current_q_values)
            
        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# --- 3. Training Loop (No changes needed here) ---
def train_agent(env, agent, episodes=100):
    portfolio_values = []
    
    for e in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0
        
        # Adjust epsilon for training, allowing more exploration in early episodes
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay
        if e == 0: # Ensure initial exploration for the very first episode
            agent.epsilon = 1.0

        while not done:
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            
            state_np = np.array(state)
            next_state_np = np.array(next_state)

            agent.remember(state_np, action, reward, next_state_np, done)
            state = next_state
            total_reward += reward
            
            agent.replay()
        
        final_portfolio_value = env.get_portfolio_value()
        portfolio_values.append(final_portfolio_value)
        
        print(f"Episode {e+1}/{episodes}, Final Portfolio Value: ${final_portfolio_value:,.2f}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}")

    return portfolio_values

# --- 4. Load your CDS Data ---
# --- IMPORTANT: Specify the path to your CSV file ---
csv_file_path = 'cds_data.csv' 

try:
    df_cds = pd.read_csv(csv_file_path, index_col='Date', parse_dates=True)
    # Ensure columns are uppercase for consistent access
    df_cds.columns = df_cds.columns.str.upper() 
    print(f"Successfully loaded data from {csv_file_path}. Columns: {df_cds.columns.tolist()}")

    # Define the index you want to trade (e.g., 'CDX_IG', 'CDX_EM', 'ITRX_MAIN', 'ITRX_XOVER')
    TARGET_CDS_INDEX = 'CDX_IG' # <--- CHANGE THIS to the index you want to trade!

    if TARGET_CDS_INDEX not in df_cds.columns:
        raise KeyError(f"The specified TARGET_CDS_INDEX '{TARGET_CDS_INDEX}' is not in the CSV file.")
    
except FileNotFoundError:
    print(f"Error: The file '{csv_file_path}' was not found.")
    print("Please ensure your CSV file is in the same directory as this script, or provide the full path.")
    print("Example CSV format:")
    print("Date,CDX_IG,CDX_EM,ITRX_Main,ITRX_XOVER")
    print("2020-01-01,100.00,200.00,50.00,300.00")
    print("... (add more data)")
    exit() # Exit if data cannot be loaded
except KeyError as e:
    print(f"Error: Missing expected column in CSV: {e}. Please check your column names.")
    print(f"Expected columns: Date, CDX_IG, CDX_EM, ITRX_Main, ITRX_XOVER (case-insensitive for access).")
    exit()

# --- 5. Run the Simulation ---
# Pass the full DataFrame and the target index column to the environment
env = CDSTradingEnv(df_cds, target_index_col=TARGET_CDS_INDEX) 
state_size = env._get_state().shape[0]
action_size = len(env.action_space)

agent = DQNAgent(state_size, action_size)

print(f"Training agent for {TARGET_CDS_INDEX} on {len(env.data)} data points...")
training_results = train_agent(env, agent, episodes=100) # You can increase episodes for more training

# --- 6. Visualization Functions ---

def plot_training_results(training_results, initial_capital, index_name):
    plt.figure(figsize=(14, 7))
    plt.plot(training_results)
    plt.title(f'Final Portfolio Value per Episode during Training ({index_name})', fontsize=16)
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Final Portfolio Value', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.axhline(y=initial_capital, color='r', linestyle='--', label='Initial Capital')
    plt.legend()
    plt.tight_layout()
    plt.show()

def plot_backtest_results(env_history, price_data, window_size, initial_capital, index_name):
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 12), sharex=True, gridspec_kw={'height_ratios': [3, 1]})

    # --- Plot 1: Spread Chart with Buy/Sell Signals ---
    dates_for_plot = price_data.index[window_size:]
    spreads_for_plot = [h['spread'] for h in env_history]

    ax1.plot(dates_for_plot, spreads_for_plot, label=f'{index_name} Spread (bps)', color='skyblue', linewidth=1.5)
    
    # Invert Y-axis for spreads to conceptually align with "price" for long positions
    # Lower spread (tighter) means higher value for a long CDS position
    ax1.invert_yaxis()
    
    # Extract signals from history
    buy_signals_x, buy_signals_y = [], []
    sell_signals_x, sell_signals_y = [], []
    
    for i, record in enumerate(env_history):
        date_index = price_data.index[i + window_size] 
        spread = record['spread']
        action = record['action']
        
        # A "buy" action means buying the "Tradable_Price" (expecting spreads to tighten/fall)
        # This occurs when the agent is currently flat (position 0) and takes action 1 (Buy)
        if action == 1 and (i == 0 or env_history[i-1]['position'] == 0) and record['position'] == 1:
             buy_signals_x.append(date_index)
             buy_signals_y.append(spread)
        # A "sell" action means liquidating a long "Tradable_Price" position (expected spreads to widen/rise)
        # This occurs when the agent is currently long (position 1) and takes action 2 (Sell)
        elif action == 2 and (i > 0 and env_history[i-1]['position'] == 1) and record['position'] == 0:
             sell_signals_x.append(date_index)
             sell_signals_y.append(spread)

    ax1.scatter(buy_signals_x, buy_signals_y, marker='^', color='green', label='Buy Signal (Spreads Tighten)', alpha=0.9, s=150, zorder=5)
    ax1.scatter(sell_signals_x, sell_signals_y, marker='v', color='red', label='Sell Signal (Spreads Widen)', alpha=0.9, s=150, zorder=5)
    
    ax1.set_title(f'{index_name} Spread with Trading Signals (Inverted Y-axis)', fontsize=16)
    ax1.set_ylabel('CDS Spread (bps)', fontsize=12)
    ax1.grid(True, linestyle='--', alpha=0.7)
    ax1.legend()
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    ax1.xaxis.set_major_locator(mdates.AutoLocator())
    fig.autofmt_xdate()

    # --- Plot 2: Portfolio Value ---
    portfolio_values = [h['portfolio_value'] for h in env_history]
    dates_for_portfolio = price_data.index[window_size:]

    ax2.plot(dates_for_portfolio, portfolio_values, label='Portfolio Value', color='blue', linewidth=2)
    ax2.axhline(y=initial_capital, color='r', linestyle='--', label='Initial Capital')
    ax2.set_title('Portfolio Value Over Time', fontsize=16)
    ax2.set_xlabel('Date', fontsize=12)
    ax2.set_ylabel('Portfolio Value', fontsize=12)
    ax2.grid(True, linestyle='--', alpha=0.7)
    ax2.legend()
    
    plt.tight_layout()
    plt.show()

# --- 7. Backtesting Module ---
def backtest_agent(env_class, agent, data_to_backtest, target_index_col):
    """
    Runs the trained agent on a given dataset and records performance.
    
    Args:
        env_class (class): The CDSTradingEnv class itself.
        agent (DQNAgent): The trained RL agent.
        data_to_backtest (pd.DataFrame): The historical data for backtesting.
        target_index_col (str): The name of the column representing the index being traded.
    
    Returns:
        float: Final portfolio value.
        list: History of the environment (for plotting).
    """
    # Create a new environment instance for backtesting to ensure clean state
    backtest_env = env_class(data_to_backtest, target_index_col=target_index_col,
                             initial_capital=env.initial_capital,
                             commission_rate=env.commission_rate,
                             spread_cost=env.spread_cost)
    
    state = backtest_env.reset()
    done = False
    
    # Set epsilon to a very low value for backtesting (exploitation only)
    agent.epsilon = 0.00 # Set to 0 for pure exploitation, or small value for tiny exploration
    
    while not done:
        action = agent.act(state)
        next_state, reward, done, _ = backtest_env.step(action)
        state = next_state

    final_value = backtest_env.get_portfolio_value()
    print(f"\nBacktesting Complete. Final Portfolio Value: ${final_value:,.2f}")
    return final_value, backtest_env.history

# --- Execute Training, Backtesting and Visualization ---

# Plot training progress
plot_training_results(training_results, env.initial_capital, TARGET_CDS_INDEX)

# Perform backtesting on the same data for demonstration (ideally, use unseen data)
print(f"\n--- Starting Backtesting for {TARGET_CDS_INDEX} ---")
final_backtest_value, backtest_history = backtest_agent(CDSTradingEnv, agent, df_cds, TARGET_CDS_INDEX)

# Visualize backtest results
plot_backtest_results(backtest_history, df_cds, env.window_size, env.initial_capital, TARGET_CDS_INDEX)
