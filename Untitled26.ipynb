import numpy as np
import pandas as pd
import random
from collections import deque
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import pandas_ta as ta # Import pandas_ta instead of talib

# Ensure reproducibility for consistent results
np.random.seed(42)
tf.random.set_seed(42)

# --- 1. Define the Trading Environment ---
class CDSTradingEnv:
    def __init__(self, data, target_index_col, initial_capital=100000, commission_rate=0.0001, spread_cost=0.0005):
        """
        Initializes the CDS Trading Environment, treating spreads as "prices" inversely.

        Args:
            data (pd.DataFrame): Historical data for CDS indexes with 'Date' as index
                                 and columns like 'CDX_IG', 'CDX_EM', etc. (spreads in bps).
            target_index_col (str): The name of the column in 'data' that represents the
                                    CDS index to be traded (e.g., 'CDX_IG').
            initial_capital (float): Starting capital for the agent.
            commission_rate (float): Commission per trade as a percentage of trade value.
            spread_cost (float): Represents bid-ask spread or slippage as a percentage.
        """
        if target_index_col not in data.columns:
            raise ValueError(f"'{target_index_col}' not found in data columns. Available: {data.columns.tolist()}")

        self.data = data.copy() # Work on a copy to avoid modifying original df
        self.target_index_col = target_index_col
        self.initial_capital = initial_capital
        self.capital = initial_capital
        self.commission_rate = commission_rate
        self.spread_cost = spread_cost
        self.shares = 0  # Number of CDS index units held (conceptual, represents exposure)
        
        # Calculate MAX_SPREAD for the whole dataset for normalization
        self.max_spread = self.data[self.target_index_col].max()
        
        # Define the "tradable price" for the agent: higher when spreads are tighter.
        # A small constant added to avoid division by zero or very large numbers if spread is near zero
        self.data['Tradable_Price'] = self.max_spread - self.data[self.target_index_col] + 1.0 
        
        self.price_history = self.data['Tradable_Price'].values
        self.spread_history = self.data[self.target_index_col].values # Keep original spreads for state features

        self.current_step = 0
        self.history = [] # To store actions, capital, and position
        self.position = 0 # 0: Hold, 1: Long, -1: Short (simplified for now, refers to "Tradable_Price")

        self.window_size = 20 # Increased window for indicators
        self.action_space = [0, 1, 2] # 0: Hold, 1: Buy (long "Tradable_Price"), 2: Sell (flatten long position)

        # Pre-calculate technical indicators and return horizons
        self._prepare_features()

        self.reset()

    def _prepare_features(self):
        # We need the original spread series for indicator calculations
        spread_series = self.data[self.target_index_col]
        
        # Return Horizons (on the raw spread data)
        self.data['Return_1D'] = spread_series.pct_change(1)
        self.data['Return_1W'] = spread_series.pct_change(5) # 5 trading days in a week
        self.data['Return_2W'] = spread_series.pct_change(10) # 10 trading days
        self.data['Return_1M'] = spread_series.pct_change(20) # 20 trading days

        # Technical Indicators using pandas_ta (directly adds to DataFrame)
        # RSI: Use `ta.rsi(series, length)`
        # `append=True` makes it add the new columns directly to `self.data`
        self.data.ta.rsi(close=spread_series, length=14, append=True)
        # MACD: Use `ta.macd(series, fast, slow, signal)`
        self.data.ta.macd(close=spread_series, fast=12, slow=26, signal=9, append=True)

        # Rename MACD columns for consistency as pandas_ta uses specific naming conventions
        # It usually names them like 'MACD_12_26_9', 'MACDs_12_26_9', 'MACDh_12_26_9'
        # We'll map them to our desired names
        self.data['RSI'] = self.data[f'RSI_14']
        self.data['MACD'] = self.data[f'MACD_12_26_9']
        self.data['MACD_Signal'] = self.data[f'MACDs_12_26_9']
        self.data['MACD_Hist'] = self.data[f'MACDh_12_26_9']

        # Fill NaNs created by technical indicators and returns (at the start of series)
        self.data.fillna(method='bfill', inplace=True) # Backfill NaNs to avoid losing early data points
        self.data.fillna(0, inplace=True) # Fill any remaining NaNs with 0

        # Features to be included in the state vector
        self.feature_cols = [
            'Tradable_Price', # Current "price"
            'Return_1D', 'Return_1W', 'Return_2W', 'Return_1M',
            'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist'
        ]
        
        # Scale features, this is important for neural networks
        self.scalers = {}
        for col in self.feature_cols:
            min_val = self.data[col].min()
            max_val = self.data[col].max()
            if max_val - min_val != 0:
                self.data[col + '_Scaled'] = (self.data[col] - min_val) / (max_val - min_val)
                self.scalers[col] = {'min': min_val, 'max': max_val}
            else:
                self.data[col + '_Scaled'] = 0.0 # If all values are the same
                self.scalers[col] = {'min': min_val, 'max': max_val} # Keep record
        
        self.scaled_feature_cols = [col + '_Scaled' for col in self.feature_cols]
        
        # Adjust starting step for window_size and largest indicator period (MACD slowperiod=26)
        self.start_step_offset = max(self.window_size, 26) 

        self.price_history = self.data['Tradable_Price'].values # Update with the new tradable price
        
        print(f"Features prepared. State size will be based on {len(self.scaled_feature_cols) * self.window_size + 1} elements.")

    def reset(self):
        self.capital = self.initial_capital
        self.shares = 0
        self.current_step = self.start_step_offset 
        self.history = []
        self.position = 0
        return self._get_state()

    def _get_state(self):
        if self.current_step < self.start_step_offset:
            return np.zeros(len(self.scaled_feature_cols) * self.window_size + 1)
        
        state_window_data = self.data.loc[self.data.index[self.current_step - self.window_size : self.current_step], self.scaled_feature_cols].values
        
        state = state_window_data.flatten()
        state = np.append(state, self.position)
        return state

    def step(self, action):
        current_tradable_price = self.price_history[self.current_step]
        current_spread = self.spread_history[self.current_step] 
        reward = 0
        done = False

        self.current_step += 1
        if self.current_step >= len(self.price_history):
            done = True

        # --- Execute Action ---
        if action == 1:  # Buy (go long on "Tradable_Price", profit if spreads tighten)
            if self.position == 0: 
                buy_amount = self.capital / (current_tradable_price * (1 + self.commission_rate + self.spread_cost))
                if buy_amount * current_tradable_price > 0: 
                    self.shares += buy_amount
                    self.capital -= buy_amount * current_tradable_price * (1 + self.commission_rate + self.spread_cost)
                    self.position = 1
        
        elif action == 2:  # Sell (liquidate long position on "Tradable_Price")
            if self.position == 1 and self.shares > 0: 
                sale_value = self.shares * current_tradable_price * (1 - self.commission_rate - self.spread_cost)
                
                entry_info = next((item for item in reversed(self.history) if item['action'] == 1 and item['position'] == 1), None)
                if entry_info:
                    entry_tradable_price = entry_info['tradable_price'] # Use recorded tradable price
                    trade_pnl = (current_tradable_price - entry_tradable_price) * self.shares
                    reward = trade_pnl / self.initial_capital * 10.0 
                    
                    if trade_pnl < 0:
                        reward -= 0.5 
                else:
                    reward = 0 
                    
                self.capital += sale_value
                self.shares = 0
                self.position = 0
        
        # --- Calculate Next State & End of Episode Reward ---
        next_state = self._get_state()

        if done:
            final_value = self.capital + self.shares * current_tradable_price * (1 - self.commission_rate)
            reward += (final_value - self.initial_capital) / self.initial_capital * 100.0 
            
            if self.shares != 0:
                current_hold_value = self.shares * current_tradable_price 
                if self.position == 1: 
                    initial_allocation = self.initial_capital - self.capital 
                    if initial_allocation > 0:
                         pnl_on_hold = (current_hold_value - initial_allocation) / initial_allocation
                         if pnl_on_hold < 0:
                            reward += pnl_on_hold * 5.0 
            
            if final_value < self.initial_capital:
                reward -= (self.initial_capital - final_value) / self.initial_capital * 20.0 
            
        self.history.append({
            'capital': self.capital,
            'shares': self.shares,
            'position': self.position,
            'spread': current_spread, 
            'tradable_price': current_tradable_price, 
            'action': action,
            'reward': reward,
            'portfolio_value': self.get_portfolio_value()
        })

        return next_state, reward, done, {}

    def get_portfolio_value(self):
        current_tradable_price = self.price_history[self.current_step-1] if self.current_step > 0 else self.price_history[0]
        return self.capital + self.shares * current_tradable_price


# --- 2. Implement the Q-Learning Agent (No changes needed here) ---
class DQNAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.95,
                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,
                 batch_size=64, memory_size=2000):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=memory_size)
        self.learning_rate = learning_rate
        self.gamma = gamma    # Discount factor for future rewards
        self.epsilon = epsilon  # Exploration-exploitation trade-off
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential()
        model.add(Dense(128, input_dim=self.state_size, activation='relu')) 
        model.add(Dense(64, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size) # Explore
        
        state_reshaped = state.reshape(1, -1) 
        act_values = self.model.predict(state_reshaped, verbose=0)
        return np.argmax(act_values[0]) # Exploit

    def replay(self):
        if len(self.memory) < self.batch_size:
            return

        minibatch = random.sample(self.memory, self.batch_size)
        states, targets = [], []
        
        for state, action, reward, next_state, done in minibatch:
            state_reshaped = state.reshape(1, -1)
            next_state_reshaped = next_state.reshape(1, -1)
            
            target = reward
            if not done:
                target = reward + self.gamma * np.amax(self.model.predict(next_state_reshaped, verbose=0)[0])
            
            current_q_values = self.model.predict(state_reshaped, verbose=0)[0]
            current_q_values[action] = target
            
            states.append(state)
            targets.append(current_q_values)
            
        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# --- 3. Training Loop (No changes needed here) ---
def train_agent(env, agent, episodes=100):
    portfolio_values = []
    
    for e in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0
        
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay
        if e == 0: 
            agent.epsilon = 1.0

        while not done:
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            
            state_np = np.array(state)
            next_state_np = np.array(next_state)

            agent.remember(state_np, action, reward, next_state_np, done)
            state = next_state
            total_reward += reward
            
            agent.replay()
        
        final_portfolio_value = env.get_portfolio_value()
        portfolio_values.append(final_portfolio_value)
        
        print(f"Episode {e+1}/{episodes}, Final Portfolio Value: ${final_portfolio_value:,.2f}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}")

    return portfolio_values

# --- 4. Load your CDS Data ---
csv_file_path = 'cds_data.csv' 

try:
    df_cds = pd.read_csv(csv_file_path, index_col='Date', parse_dates=True)
    df_cds.columns = df_cds.columns.str.upper() 
    print(f"Successfully loaded data from {csv_file_path}. Columns: {df_cds.columns.tolist()}")

    TARGET_CDS_INDEX = 'CDX_IG' # <--- CHANGE THIS to the index you want to trade!

    if TARGET_CDS_INDEX not in df_cds.columns:
        raise KeyError(f"The specified TARGET_CDS_INDEX '{TARGET_CDS_INDEX}' is not in the CSV file.")
    
except FileNotFoundError:
    print(f"Error: The file '{csv_file_path}' was not found.")
    print("Please ensure your CSV file is in the same directory as this script, or provide the full path.")
    print("Example CSV format:")
    print("Date,CDX_IG,CDX_EM,ITRX_Main,ITRX_XOVER")
    print("2020-01-01,100.00,200.00,50.00,300.00")
    print("... (add more data)")
    exit() 
except KeyError as e:
    print(f"Error: Missing expected column in CSV: {e}. Please check your column names.")
    print(f"Expected columns: Date, CDX_IG, CDX_EM, ITRX_Main, ITRX_XOVER (case-insensitive for access).")
    exit()

# --- 5. Run the Simulation ---
env = CDSTradingEnv(df_cds, target_index_col=TARGET_CDS_INDEX) 
state_size = env._get_state().shape[0]
action_size = len(env.action_space)

agent = DQNAgent(state_size, action_size)

print(f"Training agent for {TARGET_CDS_INDEX} on {len(env.data)} data points...")
training_results = train_agent(env, agent, episodes=100) 

# --- 6. Visualization Functions ---

def plot_training_results(training_results, initial_capital, index_name):
    plt.figure(figsize=(14, 7))
    plt.plot(training_results)
    plt.title(f'Final Portfolio Value per Episode during Training ({index_name})', fontsize=16)
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Final Portfolio Value', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.axhline(y=initial_capital, color='r', linestyle='--', label='Initial Capital')
    plt.legend()
    plt.tight_layout()
    plt.show()

def plot_backtest_results(env_history, price_data, window_size, initial_capital, index_name):
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 12), sharex=True, gridspec_kw={'height_ratios': [3, 1]})

    # --- Plot 1: Spread Chart with Buy/Sell Signals ---
    dates_for_plot = price_data.index[window_size:]
    spreads_for_plot = [h['spread'] for h in env_history]

    ax1.plot(dates_for_plot, spreads_for_plot, label=f'{index_name} Spread (bps)', color='skyblue', linewidth=1.5)
    
    ax1.invert_yaxis() # Invert Y-axis for spreads to conceptually align with "price" for long positions
    
    buy_signals_x, buy_signals_y = [], []
    sell_signals_x, sell_signals_y = [], []
    
    for i, record in enumerate(env_history):
        date_index = price_data.index[i + window_size] 
        spread = record['spread']
        action = record['action']
        
        if action == 1 and (i == 0 or env_history[i-1]['position'] == 0) and record['position'] == 1:
             buy_signals_x.append(date_index)
             buy_signals_y.append(spread)
        elif action == 2 and (i > 0 and env_history[i-1]['position'] == 1) and record['position'] == 0:
             sell_signals_x.append(date_index)
             sell_signals_y.append(spread)

    ax1.scatter(buy_signals_x, buy_signals_y, marker='^', color='green', label='Buy Signal (Spreads Tighten)', alpha=0.9, s=150, zorder=5)
    ax1.scatter(sell_signals_x, sell_signals_y, marker='v', color='red', label='Sell Signal (Spreads Widen)', alpha=0.9, s=150, zorder=5)
    
    ax1.set_title(f'{index_name} Spread with Trading Signals (Inverted Y-axis)', fontsize=16)
    ax1.set_ylabel('CDS Spread (bps)', fontsize=12)
    ax1.grid(True, linestyle='--', alpha=0.7)
    ax1.legend()
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    ax1.xaxis.set_major_locator(mdates.AutoLocator())
    fig.autofmt_xdate()

    # --- Plot 2: Portfolio Value ---
    portfolio_values = [h['portfolio_value'] for h in env_history]
    dates_for_portfolio = price_data.index[window_size:]

    ax2.plot(dates_for_portfolio, portfolio_values, label='Portfolio Value', color='blue', linewidth=2)
    ax2.axhline(y=initial_capital, color='r', linestyle='--', label='Initial Capital')
    ax2.set_title('Portfolio Value Over Time', fontsize=16)
    ax2.set_xlabel('Date', fontsize=12)
    ax2.set_ylabel('Portfolio Value', fontsize=12)
    ax2.grid(True, linestyle='--', alpha=0.7)
    ax2.legend()
    
    plt.tight_layout()
    plt.show()

# --- 7. Backtesting Module (No changes needed here) ---
def backtest_agent(env_class, agent, data_to_backtest, target_index_col):
    backtest_env = env_class(data_to_backtest, target_index_col=target_index_col,
                             initial_capital=env.initial_capital,
                             commission_rate=env.commission_rate,
                             spread_cost=env.spread_cost)
    
    state = backtest_env.reset()
    done = False
    
    agent.epsilon = 0.00 
    
    while not done:
        action = agent.act(state)
        next_state, reward, done, _ = backtest_env.step(action)
        state = next_state

    final_value = backtest_env.get_portfolio_value()
    print(f"\nBacktesting Complete. Final Portfolio Value: ${final_value:,.2f}")
    return final_value, backtest_env.history

# --- Execute Training, Backtesting and Visualization ---

plot_training_results(training_results, env.initial_capital, TARGET_CDS_INDEX)

print(f"\n--- Starting Backtesting for {TARGET_CDS_INDEX} ---")
final_backtest_value, backtest_history = backtest_agent(CDSTradingEnv, agent, df_cds, TARGET_CDS_INDEX)

plot_backtest_results(backtest_history, df_cds, env.window_size, env.initial_capital, TARGET_CDS_INDEX)
